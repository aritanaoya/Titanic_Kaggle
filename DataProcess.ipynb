{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataProcess.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMr+A18hThv7WTxEJZqdLvW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aritanaoya/keibaAI/blob/main/DataProcess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZvFHkARhIh7"
      },
      "source": [
        "# データ加工・前処理\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JcxceBGhe7j"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import lightgbm as lgb\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "import re\n",
        "from urllib.request import urlopen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epiVElOJh3s9"
      },
      "source": [
        "## DataProcessorクラス\n",
        "\n",
        "訓練データと出馬表データを加工する抽象クラスです。DataProcessorクラスを、この次に定義するResultsクラスとShutubaTableクラスで継承して使います。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0XcqbK2gmfs"
      },
      "source": [
        "class DataProcessor:\n",
        "    \"\"\"    \n",
        "    Attributes:\n",
        "    ----------\n",
        "    data : pd.DataFrame\n",
        "        rawデータ\n",
        "    data_p : pd.DataFrame\n",
        "        preprocessing後のデータ\n",
        "    data_h : pd.DataFrame\n",
        "        merge_horse_results後のデータ\n",
        "    data_pe : pd.DataFrame\n",
        "        merge_peds後のデータ\n",
        "    data_c : pd.DataFrame\n",
        "        process_categorical後のデータ\n",
        "    no_peds: Numpy.array\n",
        "        merge_pedsを実行した時に、血統データが存在しなかった馬のhorse_id一覧\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.data = pd.DataFrame()\n",
        "        self.data_p = pd.DataFrame()\n",
        "        self.data_h = pd.DataFrame()\n",
        "        self.data_pe = pd.DataFrame()\n",
        "        self.data_c = pd.DataFrame()\n",
        "        \n",
        "    def merge_horse_results(self, hr, n_samples_list=[5, 9, 'all']):\n",
        "        \"\"\"\n",
        "        馬の過去成績データから、\n",
        "        n_samples_listで指定されたレース分の着順と賞金の平均を追加してdata_hに返す\n",
        "        Parameters:\n",
        "        ----------\n",
        "        hr : HorseResults\n",
        "            馬の過去成績データ\n",
        "        n_samples_list : list, default [5, 9, 'all']\n",
        "            過去何レース分追加するか\n",
        "        \"\"\"\n",
        "        self.data_h = self.data_p.copy()\n",
        "        for n_samples in n_samples_list:\n",
        "            self.data_h = hr.merge_all(self.data_h, n_samples=n_samples)\n",
        "        self.data_h.drop(['開催'], axis=1, inplace=True)    \n",
        "\t    \n",
        "    def merge_peds(self, peds):\n",
        "        \"\"\"\n",
        "        5世代分血統データを追加してdata_peに返す\n",
        "        Parameters:\n",
        "        ----------\n",
        "        peds : Peds.peds_e\n",
        "            Pedsクラスで加工された血統データ。\n",
        "        \"\"\"\n",
        "\t\n",
        "        self.data_pe = \\\n",
        "            self.data_h.merge(peds, left_on='horse_id', right_index=True,\n",
        "                                                             how='left')\n",
        "        self.no_peds = self.data_pe[self.data_pe['peds_0'].isnull()]\\\n",
        "            ['horse_id'].unique()\n",
        "        if len(self.no_peds) > 0:\n",
        "            print('scrape peds at horse_id_list \"no_peds\"')\n",
        "            \n",
        "    def process_categorical(self, le_horse, le_jockey, results_m):\n",
        "        \"\"\"\n",
        "        カテゴリ変数を処理してdata_cに返す\n",
        "        Parameters:\n",
        "        ----------\n",
        "        le_horse : sklearn.preprocessing.LabelEncoder\n",
        "            horse_idを0始まりの整数に変換するLabelEncoderオブジェクト。\n",
        "        le_jockey : sklearn.preprocessing.LabelEncoder\n",
        "            jockey_idを0始まりの整数に変換するLabelEncoderオブジェクト。\n",
        "        results_m : Results.data_pe\n",
        "            ダミー変数化のとき、ResultsクラスとShutubaTableクラスで列を合わせるためのもの\n",
        "        \"\"\"\n",
        "\t\n",
        "        df = self.data_pe.copy()\n",
        "        \n",
        "        #ラベルエンコーディング。horse_id, jockey_idを0始まりの整数に変換\n",
        "        mask_horse = df['horse_id'].isin(le_horse.classes_)\n",
        "        new_horse_id = df['horse_id'].mask(mask_horse).dropna().unique()\n",
        "        le_horse.classes_ = np.concatenate([le_horse.classes_, new_horse_id])\n",
        "        df['horse_id'] = le_horse.transform(df['horse_id'])\n",
        "        mask_jockey = df['jockey_id'].isin(le_jockey.classes_)\n",
        "        new_jockey_id = df['jockey_id'].mask(mask_jockey).dropna().unique()\n",
        "        le_jockey.classes_ = np.concatenate([le_jockey.classes_, new_jockey_id])\n",
        "        df['jockey_id'] = le_jockey.transform(df['jockey_id'])\n",
        "        \n",
        "        #horse_id, jockey_idをpandasのcategory型に変換\n",
        "        df['horse_id'] = df['horse_id'].astype('category')\n",
        "        df['jockey_id'] = df['jockey_id'].astype('category')\n",
        "        \n",
        "        #そのほかのカテゴリ変数をpandasのcategory型に変換してからダミー変数化\n",
        "        #列を一定にするため\n",
        "        weathers = results_m['weather'].unique()\n",
        "        race_types = results_m['race_type'].unique()\n",
        "        ground_states = results_m['ground_state'].unique()\n",
        "        sexes = results_m['性'].unique()\n",
        "        df['weather'] = pd.Categorical(df['weather'], weathers)\n",
        "        df['race_type'] = pd.Categorical(df['race_type'], race_types)\n",
        "        df['ground_state'] = pd.Categorical(df['ground_state'], ground_states)\n",
        "        df['性'] = pd.Categorical(df['性'], sexes)\n",
        "        df = pd.get_dummies(df, columns=['weather', 'race_type', 'ground_state', '性'])\n",
        "        \n",
        "        self.data_c = df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "App-p3qBiCft"
      },
      "source": [
        "## Resultsクラス\n",
        "\n",
        "訓練に使うレース結果データを加工するクラス\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iol3e6Nuhc9I"
      },
      "source": [
        "\n",
        "class Results(DataProcessor):\n",
        "    def __init__(self, results):\n",
        "        super(Results, self).__init__()\n",
        "        self.data = results\n",
        "        \n",
        "    @classmethod\n",
        "    def read_pickle(cls, path_list):\n",
        "        df = pd.read_pickle(path_list[0])\n",
        "        for path in path_list[1:]:\n",
        "            df = update_data(df, pd.read_pickle(path))\n",
        "        return cls(df)\n",
        "    \n",
        "    @staticmethod\n",
        "    def scrape(race_id_list):\n",
        "        \"\"\"\n",
        "        レース結果データをスクレイピングする関数\n",
        "        Parameters:\n",
        "        ----------\n",
        "        race_id_list : list\n",
        "            レースIDのリスト\n",
        "        Returns:\n",
        "        ----------\n",
        "        race_results_df : pandas.DataFrame\n",
        "            全レース結果データをまとめてDataFrame型にしたもの\n",
        "        \"\"\"\n",
        "\n",
        "        #race_idをkeyにしてDataFrame型を格納\n",
        "        race_results = {}\n",
        "        for race_id in tqdm(race_id_list):\n",
        "            try:\n",
        "                url = \"https://db.netkeiba.com/race/\" + race_id\n",
        "                #メインとなるテーブルデータを取得\n",
        "                df = pd.read_html(url)[0]\n",
        "\n",
        "                html = requests.get(url)\n",
        "                html.encoding = \"EUC-JP\"\n",
        "                soup = BeautifulSoup(html.text, \"html.parser\")\n",
        "\n",
        "                #天候、レースの種類、コースの長さ、馬場の状態、日付をスクレイピング\n",
        "                texts = (\n",
        "                    soup.find(\"div\", attrs={\"class\": \"data_intro\"}).find_all(\"p\")[0].text\n",
        "                    + soup.find(\"div\", attrs={\"class\": \"data_intro\"}).find_all(\"p\")[1].text\n",
        "                )\n",
        "                info = re.findall(r'\\w+', texts)\n",
        "                for text in info:\n",
        "                    if text in [\"芝\", \"ダート\"]:\n",
        "                        df[\"race_type\"] = [text] * len(df)\n",
        "                    if \"障\" in text:\n",
        "                        df[\"race_type\"] = [\"障害\"] * len(df)\n",
        "                    if \"m\" in text:\n",
        "                        df[\"course_len\"] = [int(re.findall(r\"\\d+\", text)[0])] * len(df)\n",
        "                    if text in [\"良\", \"稍重\", \"重\", \"不良\"]:\n",
        "                        df[\"ground_state\"] = [text] * len(df)\n",
        "                    if text in [\"曇\", \"晴\", \"雨\", \"小雨\", \"小雪\", \"雪\"]:\n",
        "                        df[\"weather\"] = [text] * len(df)\n",
        "                    if \"年\" in text:\n",
        "                        df[\"date\"] = [text] * len(df)\n",
        "\n",
        "                #馬ID、騎手IDをスクレイピング\n",
        "                horse_id_list = []\n",
        "                horse_a_list = soup.find(\"table\", attrs={\"summary\": \"レース結果\"}).find_all(\n",
        "                    \"a\", attrs={\"href\": re.compile(\"^/horse\")}\n",
        "                )\n",
        "                for a in horse_a_list:\n",
        "                    horse_id = re.findall(r\"\\d+\", a[\"href\"])\n",
        "                    horse_id_list.append(horse_id[0])\n",
        "                jockey_id_list = []\n",
        "                jockey_a_list = soup.find(\"table\", attrs={\"summary\": \"レース結果\"}).find_all(\n",
        "                    \"a\", attrs={\"href\": re.compile(\"^/jockey\")}\n",
        "                )\n",
        "                for a in jockey_a_list:\n",
        "                    jockey_id = re.findall(r\"\\d+\", a[\"href\"])\n",
        "                    jockey_id_list.append(jockey_id[0])\n",
        "                df[\"horse_id\"] = horse_id_list\n",
        "                df[\"jockey_id\"] = jockey_id_list\n",
        "\n",
        "                #インデックスをrace_idにする\n",
        "                df.index = [race_id] * len(df)\n",
        "\n",
        "                race_results[race_id] = df\n",
        "                time.sleep(1)\n",
        "            #存在しないrace_idを飛ばす\n",
        "            except IndexError:\n",
        "                continue\n",
        "            #wifiの接続が切れた時などでも途中までのデータを返せるようにする\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "                break\n",
        "            #Jupyterで停止ボタンを押した時の対処\n",
        "            except:\n",
        "                break\n",
        "\n",
        "        #pd.DataFrame型にして一つのデータにまとめる\n",
        "        race_results_df = pd.concat([race_results[key] for key in race_results])\n",
        "\n",
        "        return race_results_df\n",
        "    \n",
        "    #前処理    \n",
        "    def preprocessing(self):\n",
        "        df = self.data.copy()\n",
        "\n",
        "        # 着順に数字以外の文字列が含まれているものを取り除く\n",
        "        df['着順'] = pd.to_numeric(df['着順'], errors='coerce')\n",
        "        df.dropna(subset=['着順'], inplace=True)\n",
        "        df['着順'] = df['着順'].astype(int)\n",
        "        df['rank'] = df['着順'].map(lambda x:1 if x<4 else 0)\n",
        "\n",
        "        # 性齢を性と年齢に分ける\n",
        "        df[\"性\"] = df[\"性齢\"].map(lambda x: str(x)[0])\n",
        "        df[\"年齢\"] = df[\"性齢\"].map(lambda x: str(x)[1:]).astype(int)\n",
        "\n",
        "        # 馬体重を体重と体重変化に分ける\n",
        "        df[\"体重\"] = df[\"馬体重\"].str.split(\"(\", expand=True)[0]\n",
        "        df[\"体重変化\"] = df[\"馬体重\"].str.split(\"(\", expand=True)[1].str[:-1]\n",
        "\t\n",
        "\t#errors='coerce'で、\"計不\"など変換できない時に欠損値にする\n",
        "\tdf['体重'] = pd.to_numeric(df['体重'], errors='coerce')\n",
        "\tdf['体重変化'] = pd.to_numeric(df['体重変化'], errors='coerce')\n",
        "\n",
        "        # 単勝をfloatに変換\n",
        "        df[\"単勝\"] = df[\"単勝\"].astype(float)\n",
        "\t# 距離は10の位を切り捨てる\n",
        "        df[\"course_len\"] = df[\"course_len\"].astype(float) // 100\n",
        "\n",
        "        # 不要な列を削除\n",
        "        df.drop([\"タイム\", \"着差\", \"調教師\", \"性齢\", \"馬体重\", '馬名', '騎手', '人気', '着順'],\n",
        "                axis=1, inplace=True)\n",
        "\n",
        "        df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y年%m月%d日\")\n",
        "        \n",
        "        #開催場所\n",
        "        df['開催'] = df.index.map(lambda x:str(x)[4:6])\n",
        "\n",
        "        self.data_p = df\n",
        "    \n",
        "    #カテゴリ変数の処理\n",
        "    def process_categorical(self):\n",
        "        self.le_horse = LabelEncoder().fit(self.data_pe['horse_id'])\n",
        "        self.le_jockey = LabelEncoder().fit(self.data_pe['jockey_id'])\n",
        "        super().process_categorical(self.le_horse, self.le_jockey, self.data_pe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRImjBcSiRod"
      },
      "source": [
        "## ShutubaTableクラス\n",
        "\n",
        "予測に使う出馬表データを加工するクラスです。\n",
        "ここで出馬表も取得しています"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4MQufiziOOC"
      },
      "source": [
        "class ShutubaTable(DataProcessor):\n",
        "    def __init__(self, shutuba_tables):\n",
        "        super(ShutubaTable, self).__init__()\n",
        "        self.data = shutuba_tables\n",
        "    \n",
        "    @classmethod\n",
        "    def scrape(cls, race_id_list, date):\n",
        "        data = pd.DataFrame()\n",
        "        for race_id in tqdm(race_id_list):\n",
        "            url = 'https://race.netkeiba.com/race/shutuba.html?race_id=' + race_id\n",
        "            df = pd.read_html(url)[0]\n",
        "            df = df.T.reset_index(level=0, drop=True).T\n",
        "\n",
        "            html = requests.get(url)\n",
        "            html.encoding = \"EUC-JP\"\n",
        "            soup = BeautifulSoup(html.text, \"html.parser\")\n",
        "\n",
        "            texts = soup.find('div', attrs={'class': 'RaceData01'}).text\n",
        "            texts = re.findall(r'\\w+', texts)\n",
        "            for text in texts:\n",
        "                if 'm' in text:\n",
        "                    df['course_len'] = [int(re.findall(r'\\d+', text)[0])] * len(df)\n",
        "                if text in [\"曇\", \"晴\", \"雨\", \"小雨\", \"小雪\", \"雪\"]:\n",
        "                    df[\"weather\"] = [text] * len(df)\n",
        "                if text in [\"良\", \"稍重\", \"重\"]:\n",
        "                    df[\"ground_state\"] = [text] * len(df)\n",
        "                if '不' in text:\n",
        "                    df[\"ground_state\"] = ['不良'] * len(df)\n",
        "                # 2020/12/13追加\n",
        "                if '稍' in text:\n",
        "                    df[\"ground_state\"] = ['稍重'] * len(df)\n",
        "                if '芝' in text:\n",
        "                    df['race_type'] = ['芝'] * len(df)\n",
        "                if '障' in text:\n",
        "                    df['race_type'] = ['障害'] * len(df)\n",
        "                if 'ダ' in text:\n",
        "                    df['race_type'] = ['ダート'] * len(df)\n",
        "            df['date'] = [date] * len(df)\n",
        "\n",
        "            # horse_id\n",
        "            horse_id_list = []\n",
        "            horse_td_list = soup.find_all(\"td\", attrs={'class': 'HorseInfo'})\n",
        "            for td in horse_td_list:\n",
        "                horse_id = re.findall(r'\\d+', td.find('a')['href'])[0]\n",
        "                horse_id_list.append(horse_id)\n",
        "            # jockey_id\n",
        "            jockey_id_list = []\n",
        "            jockey_td_list = soup.find_all(\"td\", attrs={'class': 'Jockey'})\n",
        "            for td in jockey_td_list:\n",
        "                jockey_id = re.findall(r'\\d+', td.find('a')['href'])[0]\n",
        "                jockey_id_list.append(jockey_id)\n",
        "            df['horse_id'] = horse_id_list\n",
        "            df['jockey_id'] = jockey_id_list\n",
        "\n",
        "            df.index = [race_id] * len(df)\n",
        "            data = data.append(df)\n",
        "            time.sleep(1)\n",
        "        return cls(data)\n",
        "             \n",
        "    #前処理            \n",
        "    def preprocessing(self):\n",
        "        df = self.data.copy()\n",
        "        \n",
        "        df[\"性\"] = df[\"性齢\"].map(lambda x: str(x)[0])\n",
        "        df[\"年齢\"] = df[\"性齢\"].map(lambda x: str(x)[1:]).astype(int)\n",
        "\n",
        "        # 馬体重を体重と体重変化に分ける\n",
        "        df = df[df[\"馬体重(増減)\"] != '--']\n",
        "        df[\"体重\"] = df[\"馬体重(増減)\"].str.split(\"(\", expand=True)[0].astype(int)\n",
        "        df[\"体重変化\"] = df[\"馬体重(増減)\"].str.split(\"(\", expand=True)[1].str[:-1]\n",
        "        # 2020/12/13追加：増減が「前計不」などのとき欠損値にする\n",
        "        df['体重変化'] = pd.to_numeric(df['体重変化'], errors='coerce')\n",
        "        \n",
        "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "        \n",
        "        df['枠'] = df['枠'].astype(int)\n",
        "        df['馬番'] = df['馬番'].astype(int)\n",
        "        df['斤量'] = df['斤量'].astype(int)\n",
        "        \n",
        "        df['開催'] = df.index.map(lambda x:str(x)[4:6])\n",
        "\n",
        "        # 使用する列を選択\n",
        "        df = df[['枠', '馬番', '斤量', 'course_len', 'weather','race_type',\n",
        "        'ground_state', 'date', 'horse_id', 'jockey_id', '性', '年齢',\n",
        "       '体重', '体重変化', '開催']]\n",
        "        \n",
        "        self.data_p = df.rename(columns={'枠': '枠番'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-yKpFXi8N66"
      },
      "source": [
        "## HorseResultsクラス\n",
        "\n",
        "馬の過去成績データを保持するクラスです。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrOB1MJkhs1Q"
      },
      "source": [
        "class HorseResults:\n",
        "    def __init__(self, horse_results):\n",
        "        self.horse_results = horse_results[['日付', '着順', '賞金', '着差', '通過', '開催', '距離']]\n",
        "        self.preprocessing()\n",
        "    \n",
        "    @classmethod\n",
        "    def read_pickle(cls, path_list):\n",
        "        df = pd.read_pickle(path_list[0])\n",
        "        for path in path_list[1:]:\n",
        "            df = update_data(df, pd.read_pickle(path))\n",
        "        return cls(df)\n",
        "    \n",
        "    @staticmethod\n",
        "    def scrape(horse_id_list):\n",
        "        \"\"\"\n",
        "        馬の過去成績データをスクレイピングする関数\n",
        "        Parameters:\n",
        "        ----------\n",
        "        horse_id_list : list\n",
        "            馬IDのリスト\n",
        "        Returns:\n",
        "        ----------\n",
        "        horse_results_df : pandas.DataFrame\n",
        "            全馬の過去成績データをまとめてDataFrame型にしたもの\n",
        "        \"\"\"\n",
        "\n",
        "        #horse_idをkeyにしてDataFrame型を格納\n",
        "        horse_results = {}\n",
        "        for horse_id in tqdm(horse_id_list):\n",
        "            try:\n",
        "                url = 'https://db.netkeiba.com/horse/' + horse_id\n",
        "                df = pd.read_html(url)[3]\n",
        "                #受賞歴がある馬の場合、3番目に受賞歴テーブルが来るため、4番目のデータを取得する\n",
        "                if df.columns[0]=='受賞歴':\n",
        "                    df = pd.read_html(url)[4]\n",
        "                df.index = [horse_id] * len(df)\n",
        "                horse_results[horse_id] = df\n",
        "                time.sleep(1)\n",
        "            except IndexError:\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "                break\n",
        "            except:\n",
        "                break\n",
        "\n",
        "        #pd.DataFrame型にして一つのデータにまとめる        \n",
        "        horse_results_df = pd.concat([horse_results[key] for key in horse_results])\n",
        "\n",
        "        return horse_results_df\n",
        "    \n",
        "    def preprocessing(self):\n",
        "        df = self.horse_results.copy()\n",
        "\n",
        "        # 着順に数字以外の文字列が含まれているものを取り除く\n",
        "        df['着順'] = pd.to_numeric(df['着順'], errors='coerce')\n",
        "        df.dropna(subset=['着順'], inplace=True)\n",
        "        df['着順'] = df['着順'].astype(int)\n",
        "\n",
        "        df[\"date\"] = pd.to_datetime(df[\"日付\"])\n",
        "        df.drop(['日付'], axis=1, inplace=True)\n",
        "        \n",
        "        #賞金のNaNを0で埋める\n",
        "        df['賞金'].fillna(0, inplace=True)\n",
        "        \n",
        "        #1着の着差を0にする\n",
        "        df['着差'] = df['着差'].map(lambda x: 0 if x<0 else x)\n",
        "        \n",
        "        #レース展開データ\n",
        "        #n=1: 最初のコーナー位置, n=4: 最終コーナー位置\n",
        "        def corner(x, n):\n",
        "            if type(x) != str:\n",
        "                return x\n",
        "            elif n==4:\n",
        "                return int(re.findall(r'\\d+', x)[-1])\n",
        "            elif n==1:\n",
        "                return int(re.findall(r'\\d+', x)[0])\n",
        "        df['first_corner'] = df['通過'].map(lambda x: corner(x, 1))\n",
        "        df['final_corner'] = df['通過'].map(lambda x: corner(x, 4))\n",
        "        \n",
        "        df['final_to_rank'] = df['final_corner'] - df['着順']\n",
        "        df['first_to_rank'] = df['first_corner'] - df['着順']\n",
        "        df['first_to_final'] = df['first_corner'] - df['final_corner']\n",
        "        \n",
        "        #開催場所\n",
        "        df['開催'] = df['開催'].str.extract(r'(\\D+)')[0].map(place_dict).fillna('11')\n",
        "        #race_type\n",
        "        df['race_type'] = df['距離'].str.extract(r'(\\D+)')[0].map(race_type_dict)\n",
        "        #距離は10の位を切り捨てる\n",
        "        df['course_len'] = df['距離'].str.extract(r'(\\d+)').astype(int) // 100\n",
        "        df.drop(['距離'], axis=1, inplace=True)\n",
        "        #インデックス名を与える\n",
        "        df.index.name = 'horse_id'\n",
        "        \n",
        "        self.horse_results = df\n",
        "        self.target_list = ['着順', '賞金', '着差', 'first_corner', 'final_corner',\n",
        "                            'first_to_rank', 'first_to_final','final_to_rank']\n",
        "    \n",
        "    #n_samplesレース分馬ごとに平均する\n",
        "    def average(self, horse_id_list, date, n_samples='all'):\n",
        "        target_df = self.horse_results.query('index in @horse_id_list')\n",
        "        \n",
        "        #過去何走分取り出すか指定\n",
        "        if n_samples == 'all':\n",
        "            filtered_df = target_df[target_df['date'] < date]\n",
        "        elif n_samples > 0:\n",
        "            filtered_df = target_df[target_df['date'] < date].\\\n",
        "                sort_values('date', ascending=False).groupby(level=0).head(n_samples)\n",
        "        else:\n",
        "            raise Exception('n_samples must be >0')\n",
        "        \n",
        "\t#集計して辞書型に入れる\n",
        "        self.average_dict = {}\n",
        "        self.average_dict['non_category'] = filtered_df.groupby(level=0)[self.target_list].mean()\\\n",
        "            .add_suffix('_{}R'.format(n_samples))\n",
        "        for column in ['course_len', 'race_type', '開催']:\n",
        "            self.average_dict[column] = filtered_df.groupby(['horse_id', column])\\\n",
        "                [self.target_list].mean().add_suffix('_{}_{}R'.format(column, n_samples))\n",
        "    \n",
        "    def merge(self, results, date, n_samples='all'):\n",
        "        df = results[results['date']==date]\n",
        "        horse_id_list = df['horse_id']\n",
        "        self.average(horse_id_list, date, n_samples)\n",
        "        merged_df = df.merge(self.average_dict['non_category'], left_on='horse_id',\n",
        "                             right_index=True, how='left')\n",
        "        for column in ['course_len','race_type', '開催']:\n",
        "            merged_df = merged_df.merge(self.average_dict[column], \n",
        "                                        left_on=['horse_id', column],\n",
        "                                        right_index=True, how='left')\n",
        "        return merged_df\n",
        "    \n",
        "    def merge_all(self, results, n_samples='all'):\n",
        "        date_list = results['date'].unique()\n",
        "        merged_df = pd.concat([self.merge(results, date, n_samples) for date in tqdm(date_list)])\n",
        "        return merged_df\n",
        "\n",
        "#開催場所をidに変換するための辞書型\n",
        "place_dict = {\n",
        "    '札幌':'01',  '函館':'02',  '福島':'03',  '新潟':'04',  '東京':'05', \n",
        "    '中山':'06',  '中京':'07',  '京都':'08',  '阪神':'09',  '小倉':'10'\n",
        "}\n",
        "\n",
        "#レースタイプをレース結果データと整合させるための辞書型\n",
        "race_type_dict = {\n",
        "    '芝': '芝', 'ダ': 'ダート', '障': '障害'\n",
        "}\n",
        "view rawHorseResults.py hosted with ❤ by GitHub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-lxx-m39FBD"
      },
      "source": [
        "## Pedsクラス\n",
        "\n",
        "血統データを保持するクラスです。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qrpYEVl9Kso"
      },
      "source": [
        "\n",
        "class Peds:\n",
        "    def __init__(self, peds):\n",
        "        self.peds = peds\n",
        "        self.peds_e = pd.DataFrame() #after label encoding and transforming into category\n",
        "    \n",
        "    @classmethod\n",
        "    def read_pickle(cls, path_list):\n",
        "        df = pd.read_pickle(path_list[0])\n",
        "        for path in path_list[1:]:\n",
        "            df = update_data(df, pd.read_pickle(path))\n",
        "        return cls(df)\n",
        "    \n",
        "    @staticmethod\n",
        "    def scrape(horse_id_list):\n",
        "        \"\"\"\n",
        "        血統データをスクレイピングする関数\n",
        "        Parameters:\n",
        "        ----------\n",
        "        horse_id_list : list\n",
        "            馬IDのリスト\n",
        "        Returns:\n",
        "        ----------\n",
        "        peds_df : pandas.DataFrame\n",
        "            全血統データをまとめてDataFrame型にしたもの\n",
        "        \"\"\"\n",
        "\n",
        "        peds_dict = {}\n",
        "        for horse_id in tqdm(horse_id_list):\n",
        "            try:\n",
        "                url = \"https://db.netkeiba.com/horse/ped/\" + horse_id\n",
        "                df = pd.read_html(url)[0]\n",
        "\n",
        "                #重複を削除して1列のSeries型データに直す\n",
        "                generations = {}\n",
        "                for i in reversed(range(5)):\n",
        "                    generations[i] = df[i]\n",
        "                    df.drop([i], axis=1, inplace=True)\n",
        "                    df = df.drop_duplicates()\n",
        "                ped = pd.concat([generations[i] for i in range(5)]).rename(horse_id)\n",
        "\n",
        "                peds_dict[horse_id] = ped.reset_index(drop=True)\n",
        "                time.sleep(1)\n",
        "            except IndexError:\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "                break\n",
        "            except:\n",
        "                break\n",
        "\n",
        "        #列名をpeds_0, ..., peds_61にする\n",
        "        peds_df = pd.concat([peds_dict[key] for key in peds_dict], axis=1).T.add_prefix('peds_')\n",
        "\n",
        "        return peds_df\n",
        "    \n",
        "    def encode(self):\n",
        "        df = self.peds.copy()\n",
        "        for column in df.columns:\n",
        "            df[column] = LabelEncoder().fit_transform(df[column].fillna('Na'))\n",
        "        self.peds_e = df.astype('category')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky7QBi3Q9Xnz"
      },
      "source": [
        "## データ加工の流れ\n",
        "\n",
        "まずはResultsクラス内のpreprocessing関数で、着順を0or1に直したり不要な列を削除したりしています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGHwY5PG9VKH"
      },
      "source": [
        "r = Results.read_pickle(['results.pickle'])\n",
        "r.preprocessing()\n",
        "r.data_p.head() #jupyterで出力"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saOfJ5qX9c-8"
      },
      "source": [
        "次に、馬の直近5レース、9レース、全レースの過去成績の平均を列に加えます。DataProcessorクラスから継承したmerge_horse_results関数を使います。引数にはHorseResultsクラスのオブジェクトと何走分を考えるかのリストを入れます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PL4iTqaA9dTv"
      },
      "source": [
        "r.merge_horse_results(hr, n_samples_list=[5, 9, 'all'])\n",
        "r.data_h.head() #jupyterで出力"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tri_njOu9dga"
      },
      "source": [
        "次に5世代分の血統データを追加するのですが、merge_horse_resultsと同様に、継承したDataProcessorクラスのmerge_peds関数を使います。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1brxyDF09doG"
      },
      "source": [
        "r.merge_peds(p.peds_e)\n",
        "r.data_pe.head() #jupyterで出力"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHDGZKm79tDt"
      },
      "source": [
        "最後にカテゴリ変数の処理として、次の2つのことをします。\n",
        "\n",
        "1. horse_idとjockey_idを血統データと同様にPandasのcategory型に変更する\n",
        "2. 天気、レース種別、馬場の状態、馬の性別をダミー変数化\n",
        "\n",
        "レース結果データについては、Resultsクラス内でprocess_categorical関数をオーバーライドします。この時、出馬表データのhorse_idとjockey_idを処理する時のために、Resultsクラス内のle_horseとle_jockeyという変数に、LabelEncoderオブジェクトを保持しておきます。ダミー変数化のところがなぜ複雑になっているかは、次の出馬表データの加工のところで説明します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VCF0DWf9tL4"
      },
      "source": [
        "r.process_categorical() #r.le_horse, r.le_jockeyに対応関係が保存される"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWZ59eV496bU"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkLUkR1796jn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}