{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP55XlyZogiX4plU6G1xb5E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aritanaoya/keibaAI/blob/main/scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZxY4dZ9Vd3b"
      },
      "source": [
        "# スクレイピング\n",
        "\n",
        "### 以下4種類のデータをスクレイピングする。\n",
        "\n",
        "- レース結果データ\tResults.scrape()\tresults.pickle\n",
        "- 馬の過去成績データ\tHorseResults.scrape()\thorse_results.pickle\n",
        "- 血統データ\tPeds.scrape()\tpeds.pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCnJhNmtbfLz"
      },
      "source": [
        "## モジュールのインポート"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4PL1VGybPxN"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "import re\n",
        "from urllib.request import urlopen"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-DtP4w5XZAA"
      },
      "source": [
        "## Results.scrape()\n",
        "\n",
        "メインとなる訓練データである、レース結果データをスクレイピングするメソッド"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1yPOL1nVZ_M"
      },
      "source": [
        "class Results:\n",
        "    @staticmethod\n",
        "    def scrape(race_id_list):\n",
        "        \"\"\"\n",
        "        レース結果データをスクレイピングする関数\n",
        "\n",
        "        Parameters:\n",
        "        ----------\n",
        "        race_id_list : list\n",
        "            レースIDのリスト\n",
        "\n",
        "        Returns:\n",
        "        ----------\n",
        "        race_results_df : pandas.DataFrame\n",
        "            全レース結果データをまとめてDataFrame型にしたもの\n",
        "        \"\"\"\n",
        "\n",
        "        #race_idをkeyにしてDataFrame型を格納\n",
        "        race_results = {}\n",
        "        for race_id in tqdm(race_id_list):\n",
        "          time.sleep(1)\n",
        "          try:\n",
        "            url = \"https://db.netkeiba.com/race/\" + race_id\n",
        "            #メインとなるテーブルデータを取得\n",
        "            df = pd.read_html(url)[0]\n",
        "\n",
        "            html = requests.get(url)\n",
        "            html.encoding = \"EUC-JP\"\n",
        "            soup = BeautifulSoup(html.text, \"html.parser\")\n",
        "\n",
        "            #天候、レースの種類、コースの長さ、馬場の状態、日付をスクレイピング\n",
        "            #NetKeibaの例     2歳未勝利\n",
        "            #                 芝右1800m / 天候 : 曇 / 芝 : 良 / 発走 : 09:50\n",
        "            texts = (\n",
        "                soup.find(\"div\", attrs={\"class\": \"data_intro\"}).find_all(\"p\")[0].text\n",
        "                + soup.find(\"div\", attrs={\"class\": \"data_intro\"}).find_all(\"p\")[1].text\n",
        "            )\n",
        "            info = re.findall(r'\\w+', texts)\n",
        "            for text in info:\n",
        "                if text in [\"芝\", \"ダート\"]:\n",
        "                    df[\"race_type\"] = [text] * len(df)\n",
        "                if \"障\" in text:\n",
        "                    df[\"race_type\"] = [\"障害\"] * len(df)\n",
        "                if \"m\" in text:\n",
        "                    df[\"course_len\"] = [int(re.findall(r\"\\d+\", text)[0])] * len(df)\n",
        "                if text in [\"良\", \"稍重\", \"重\", \"不良\"]:\n",
        "                    df[\"ground_state\"] = [text] * len(df)\n",
        "                if text in [\"曇\", \"晴\", \"雨\", \"小雨\", \"小雪\", \"雪\"]:\n",
        "                    df[\"weather\"] = [text] * len(df)\n",
        "                if \"年\" in text:\n",
        "                    df[\"date\"] = [text] * len(df)\n",
        "\n",
        "            #馬ID、騎手IDをスクレイピング\n",
        "            horse_id_list = []\n",
        "            horse_a_list = soup.find(\"table\", attrs={\"summary\": \"レース結果\"}).find_all(\n",
        "                \"a\", attrs={\"href\": re.compile(\"^/horse\")}\n",
        "            )\n",
        "            for a in horse_a_list:\n",
        "                horse_id = re.findall(r\"\\d+\", a[\"href\"])\n",
        "                horse_id_list.append(horse_id[0])\n",
        "            jockey_id_list = []\n",
        "            jockey_a_list = soup.find(\"table\", attrs={\"summary\": \"レース結果\"}).find_all(\n",
        "                \"a\", attrs={\"href\": re.compile(\"^/jockey\")}\n",
        "            )\n",
        "            for a in jockey_a_list:\n",
        "                jockey_id = re.findall(r\"\\d+\", a[\"href\"])\n",
        "                jockey_id_list.append(jockey_id[0])\n",
        "            df[\"horse_id\"] = horse_id_list\n",
        "            df[\"jockey_id\"] = jockey_id_list\n",
        "\n",
        "            #インデックスをrace_idにする\n",
        "            df.index = [race_id] * len(df)\n",
        "\n",
        "            race_results[race_id] = df\n",
        "          #存在しないrace_idを飛ばす\n",
        "          except IndexError:\n",
        "            continue\n",
        "          #wifiの接続が切れた時などでも途中までのデータを返せるようにする\n",
        "          except Exception as e:\n",
        "            print(e)\n",
        "            break\n",
        "          #Jupyterで停止ボタンを押した時の対処\n",
        "          except:\n",
        "            break\n",
        "\n",
        "        #pd.DataFrame型にして一つのデータにまとめる\n",
        "        race_results_df = pd.concat([race_results[key] for key in race_results])\n",
        "\n",
        "        return race_results_df"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPFvvEu-aKR1"
      },
      "source": [
        "実際に2020年のデータを取得\n",
        "\n",
        "- 年次\n",
        "- 場所\n",
        "  - 01: 札幌 02: 函館 03: 福島 04: 新潟 05: 東京06:中山07: 中京 08: 京都 09:阪神 10: 小倉\n",
        "- レースの開催回数\n",
        "- 開催日\n",
        "- round"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaXtYpkjV6vG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08be1204-db66-40d1-dae4-a4b3e41899ce"
      },
      "source": [
        "race_id_list = []\n",
        "year = \"2021\"\n",
        "for place in range(1,10,1):\n",
        "  for kai in range(1, 13, 1):\n",
        "    for day in range(1, 13, 1):\n",
        "      for r in range(1, 13, 1):\n",
        "          race_id = year + str(place).zfill(2) + str(kai).zfill(2) + str(day).zfill(2) + str(r).zfill(2)\n",
        "          race_id_list.append(race_id)\n",
        "    \n",
        "len(race_id_list)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15552"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bK4nqMl5uXMj"
      },
      "source": [
        "results = Results.scrape(race_id_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHT_aVKUcl7R"
      },
      "source": [
        " データの保存"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppbDxRefbDtg"
      },
      "source": [
        "results.to_pickle('results_tmp.pickle')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLoAeyyMd8uU"
      },
      "source": [
        "## HorseResults.scrape()\n",
        "\n",
        "馬の過去成績データをスクレイピングするメソッド\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8Os322KeA-7"
      },
      "source": [
        "#馬の過去成績データを処理するクラス\n",
        "class HorseResults:\n",
        "  @staticmethod\n",
        "  def scrape(horse_id_list):\n",
        "      \"\"\"\n",
        "      馬の過去成績データをスクレイピングする関数\n",
        "\n",
        "      Parameters:\n",
        "      ----------\n",
        "      horse_id_list : list\n",
        "          馬IDのリスト\n",
        "\n",
        "      Returns:\n",
        "      ----------\n",
        "      horse_results_df : pandas.DataFrame\n",
        "          全馬の過去成績データをまとめてDataFrame型にしたもの\n",
        "      \"\"\"\n",
        "\n",
        "      #horse_idをkeyにしてDataFrame型を格納\n",
        "      horse_results = {}\n",
        "      for horse_id in tqdm(horse_id_list):\n",
        "          try:\n",
        "              url = 'https://db.netkeiba.com/horse/' + horse_id\n",
        "              df = pd.read_html(url)[3]\n",
        "              #受賞歴がある馬の場合、3番目に受賞歴テーブルが来るため、4番目のデータを取得する\n",
        "              if df.columns[0]=='受賞歴':\n",
        "                  df = pd.read_html(url)[4]\n",
        "              df.index = [horse_id] * len(df)\n",
        "              horse_results[horse_id] = df\n",
        "              time.sleep(1)\n",
        "          except IndexError:\n",
        "              continue\n",
        "          except Exception as e:\n",
        "              print(e)\n",
        "              break\n",
        "          except:\n",
        "              break\n",
        "\n",
        "      #pd.DataFrame型にして一つのデータにまとめる        \n",
        "      horse_results_df = pd.concat([horse_results[key] for key in horse_results])\n",
        "\n",
        "      return horse_results_df"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUl6Dn8HS5Qi"
      },
      "source": [
        "上記で取得した馬idを元に馬の成績を保存"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyWAUdEwePGm"
      },
      "source": [
        "horse_id_list = results['horse_id'].unique()\n",
        "horse_results = HorseResults.scrape(horse_id_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO5eI2L-SzGj"
      },
      "source": [
        "馬成績の保存"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ushAoqcUOrgL"
      },
      "source": [
        "horse_results.to_pickle('horse_results_tmp.pickle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6ykmiwETI31"
      },
      "source": [
        "## Peds.scrape()\n",
        "\n",
        "馬ごとに3世代分の血統データをスクレイピングする"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwxxd5JETIv2"
      },
      "source": [
        "#血統データを処理するクラス\n",
        "class Peds:\n",
        "    @staticmethod\n",
        "    def scrape(horse_id_list):\n",
        "        \"\"\"\n",
        "        血統データをスクレイピングする関数\n",
        "\n",
        "        Parameters:\n",
        "        ----------\n",
        "        horse_id_list : list\n",
        "            馬IDのリスト\n",
        "\n",
        "        Returns:\n",
        "        ----------\n",
        "        peds_df : pandas.DataFrame\n",
        "            全血統データをまとめてDataFrame型にしたもの\n",
        "        \"\"\"\n",
        "\n",
        "        peds_dict = {}\n",
        "        for horse_id in tqdm(horse_id_list):\n",
        "            try:\n",
        "            url = \"https://db.netkeiba.com/horse/ped/\" + horse_id\n",
        "            df = pd.read_html(url)[0]\n",
        "\n",
        "            #重複を削除して1列のSeries型データに直す\n",
        "            generations = {}\n",
        "            for i in reversed(range(3)):\n",
        "                generations[i] = df[i]\n",
        "                df.drop([i], axis=1, inplace=True)\n",
        "                df = df.drop_duplicates()\n",
        "            ped = pd.concat([generations[i] for i in range(5)]).rename(horse_id)\n",
        "\n",
        "            peds_dict[horse_id] = ped.reset_index(drop=True)\n",
        "            time.sleep(1)\n",
        "            except IndexError:\n",
        "            continue\n",
        "            except Exception as e:\n",
        "            print(e)\n",
        "            break\n",
        "            except:\n",
        "            break\n",
        "\n",
        "    #列名をpeds_0, ..., peds_61にする\n",
        "    peds_df = pd.concat([peds_dict[key] for key in peds_dict], axis=1).T.add_prefix('peds_')\n",
        "\n",
        "    return peds_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZSZXMJ9ToNe"
      },
      "source": [
        "血統データを取得"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYTISqByTyJT"
      },
      "source": [
        "horse_id_list = results['horse_id'].unique()\n",
        "peds_results = Peds.scrape(horse_id_list)\n",
        "peds_results #jupyterで出力"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxLCzpsIUAIh"
      },
      "source": [
        "データの保存"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_pv5ShST_6v"
      },
      "source": [
        "peds_results.to_pickle('peds_results_tmp.pickle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnCMiE1aVcwk"
      },
      "source": [
        "## update_data関数\n",
        "\n",
        "馬の過去成績データなどを新しくスクレイピングして古いデータに追加する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMTCbBVoVdFb"
      },
      "source": [
        "def update_data(old, new):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    ----------\n",
        "    old : pandas.DataFrame\n",
        "        古いデータ\n",
        "    new : pandas.DataFrame\n",
        "        新しいデータ\n",
        "    \"\"\"\n",
        "\n",
        "    filtered_old = old[~old.index.isin(new.index)]\n",
        "    return pd.concat([filtered_old, new])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RApFG9rntg5B"
      },
      "source": [
        "update_data(results.pickle,results_tmp.pickle)\n",
        "update_data(horse_results.pickle,horse_results_tmp.pickle)\n",
        "update_data(peds_results.pickle,peds_results_tmp.pickle)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}