{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMOrEMQBdGvKI2i12x4ZKFO"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZxY4dZ9Vd3b"
      },
      "source": [
        "# スクレイピング\n",
        "\n",
        "### 以下4種類のデータをスクレイピングする。\n",
        "\n",
        "- レース結果データ\tResults.scrape()\tresults.pickle\n",
        "- 馬の過去成績データ\tHorseResults.scrape()\thorse_results.pickle\n",
        "- 血統データ\tPeds.scrape()\tpeds.pickle\n",
        "- 払い戻し表データ\tReturn.scrape()\treturn_tables.pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-DtP4w5XZAA"
      },
      "source": [
        "## Results.scrape()\n",
        "\n",
        "メインとなる訓練データである、レース結果データをスクレイピングするメソッド"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1yPOL1nVZ_M"
      },
      "source": [
        "class Results:\n",
        "    @staticmethod\n",
        "    def scrape(race_id_list):\n",
        "        \"\"\"\n",
        "        レース結果データをスクレイピングする関数\n",
        "\n",
        "        Parameters:\n",
        "        ----------\n",
        "        race_id_list : list\n",
        "            レースIDのリスト\n",
        "\n",
        "        Returns:\n",
        "        ----------\n",
        "        race_results_df : pandas.DataFrame\n",
        "            全レース結果データをまとめてDataFrame型にしたもの\n",
        "        \"\"\"\n",
        "\n",
        "        #race_idをkeyにしてDataFrame型を格納\n",
        "        race_results = {}\n",
        "        for race_id in tqdm(race_id_list):\n",
        "          time.sleep(1)\n",
        "          try:\n",
        "            url = \"https://db.netkeiba.com/race/\" + race_id\n",
        "            #メインとなるテーブルデータを取得\n",
        "            df = pd.read_html(url)[0]\n",
        "\n",
        "            html = requests.get(url)\n",
        "            html.encoding = \"EUC-JP\"\n",
        "            soup = BeautifulSoup(html.text, \"html.parser\")\n",
        "\n",
        "            #天候、レースの種類、コースの長さ、馬場の状態、日付をスクレイピング\n",
        "            #NetKeibaの例     2歳未勝利\n",
        "            #                 芝右1800m / 天候 : 曇 / 芝 : 良 / 発走 : 09:50\n",
        "            texts = (\n",
        "                soup.find(\"div\", attrs={\"class\": \"data_intro\"}).find_all(\"p\")[0].text\n",
        "                + soup.find(\"div\", attrs={\"class\": \"data_intro\"}).find_all(\"p\")[1].text\n",
        "            )\n",
        "            info = re.findall(r'\\w+', texts)\n",
        "            for text in info:\n",
        "                if text in [\"芝\", \"ダート\"]:\n",
        "                    df[\"race_type\"] = [text] * len(df)\n",
        "                if \"障\" in text:\n",
        "                    df[\"race_type\"] = [\"障害\"] * len(df)\n",
        "                if \"m\" in text:\n",
        "                    df[\"course_len\"] = [int(re.findall(r\"\\d+\", text)[0])] * len(df)\n",
        "                if text in [\"良\", \"稍重\", \"重\", \"不良\"]:\n",
        "                    df[\"ground_state\"] = [text] * len(df)\n",
        "                if text in [\"曇\", \"晴\", \"雨\", \"小雨\", \"小雪\", \"雪\"]:\n",
        "                    df[\"weather\"] = [text] * len(df)\n",
        "                if \"年\" in text:\n",
        "                    df[\"date\"] = [text] * len(df)\n",
        "\n",
        "            #馬ID、騎手IDをスクレイピング\n",
        "            horse_id_list = []\n",
        "            horse_a_list = soup.find(\"table\", attrs={\"summary\": \"レース結果\"}).find_all(\n",
        "                \"a\", attrs={\"href\": re.compile(\"^/horse\")}\n",
        "            )\n",
        "            for a in horse_a_list:\n",
        "                horse_id = re.findall(r\"\\d+\", a[\"href\"])\n",
        "                horse_id_list.append(horse_id[0])\n",
        "            jockey_id_list = []\n",
        "            jockey_a_list = soup.find(\"table\", attrs={\"summary\": \"レース結果\"}).find_all(\n",
        "                \"a\", attrs={\"href\": re.compile(\"^/jockey\")}\n",
        "            )\n",
        "            for a in jockey_a_list:\n",
        "                jockey_id = re.findall(r\"\\d+\", a[\"href\"])\n",
        "                jockey_id_list.append(jockey_id[0])\n",
        "            df[\"horse_id\"] = horse_id_list\n",
        "            df[\"jockey_id\"] = jockey_id_list\n",
        "\n",
        "            #インデックスをrace_idにする\n",
        "            df.index = [race_id] * len(df)\n",
        "\n",
        "            race_results[race_id] = df\n",
        "          #存在しないrace_idを飛ばす\n",
        "          except IndexError:\n",
        "            continue\n",
        "          #wifiの接続が切れた時などでも途中までのデータを返せるようにする\n",
        "          except Exception as e:\n",
        "            print(e)\n",
        "            break\n",
        "          #Jupyterで停止ボタンを押した時の対処\n",
        "          except:\n",
        "            break\n",
        "\n",
        "        #pd.DataFrame型にして一つのデータにまとめる\n",
        "        race_results_df = pd.concat([race_results[key] for key in race_results])\n",
        "\n",
        "        return race_results_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaXtYpkjV6vG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}